{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Dataframes\n",
    "import re  # Regular expressions \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  # Split text into words\n",
    "from nltk.corpus import stopwords  # Lists of unimportant words\n",
    "from collections import Counter, defaultdict  # Count word frequency & provide more versatile dicts\n",
    "from pandas.core.common import flatten  # Collapse lists of lists\n",
    "from nltk.stem.wordnet import WordNetLemmatizer  # Reduce terms to their root\n",
    "from nltk import pos_tag  # Tag words with parts of speech\n",
    "import seaborn as sns  # Visualisations\n",
    "import matplotlib.pyplot as plt  # Visualisations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Convert text to TF-IDF representations\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # Check similarities between vectors\n",
    "from textwrap import wrap  # format long text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment out the following lines if you haven't downloaded the NLTK packages (you only need to do this once)\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn</th>\n",
       "      <th>language_code</th>\n",
       "      <th>description</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>book_id</th>\n",
       "      <th>title</th>\n",
       "      <th>num_pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0743294297</td>\n",
       "      <td>eng</td>\n",
       "      <td>Addie Downs and Valerie Adler were eight when ...</td>\n",
       "      <td>9780743294294</td>\n",
       "      <td>6066819</td>\n",
       "      <td>Best Friends Forever</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0842379428</td>\n",
       "      <td>eng</td>\n",
       "      <td>What is Heaven really going to be like? What w...</td>\n",
       "      <td>9780842379427</td>\n",
       "      <td>89376</td>\n",
       "      <td>Heaven</td>\n",
       "      <td>533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0590417010</td>\n",
       "      <td>eng</td>\n",
       "      <td>In Newbery Medalist Cynthia Rylant's classic b...</td>\n",
       "      <td>9780590417013</td>\n",
       "      <td>89378</td>\n",
       "      <td>Dog Heaven</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8495971933</td>\n",
       "      <td>spa</td>\n",
       "      <td>En una hermosa mansion a orillas del Mediterra...</td>\n",
       "      <td>9788495971937</td>\n",
       "      <td>2008910</td>\n",
       "      <td>Buenos días, tristeza</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1479174661</td>\n",
       "      <td>eng</td>\n",
       "      <td>Arrianna Williams is an ordinary 25 yr. old wo...</td>\n",
       "      <td>9781479174669</td>\n",
       "      <td>16037548</td>\n",
       "      <td>Untold Secrets: Fire &amp; Ice</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         isbn language_code  \\\n",
       "0  0743294297           eng   \n",
       "1  0842379428           eng   \n",
       "2  0590417010           eng   \n",
       "3  8495971933           spa   \n",
       "4  1479174661           eng   \n",
       "\n",
       "                                         description         isbn13   book_id  \\\n",
       "0  Addie Downs and Valerie Adler were eight when ...  9780743294294   6066819   \n",
       "1  What is Heaven really going to be like? What w...  9780842379427     89376   \n",
       "2  In Newbery Medalist Cynthia Rylant's classic b...  9780590417013     89378   \n",
       "3  En una hermosa mansion a orillas del Mediterra...  9788495971937   2008910   \n",
       "4  Arrianna Williams is an ordinary 25 yr. old wo...  9781479174669  16037548   \n",
       "\n",
       "                        title  num_pages  \n",
       "0        Best Friends Forever        368  \n",
       "1                      Heaven        533  \n",
       "2                  Dog Heaven         40  \n",
       "3       Buenos días, tristeza        184  \n",
       "4  Untold Secrets: Fire & Ice        168  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe by reading the goodreads_books.csv file\n",
    "books = pd.read_csv('Datasets/goodreads_books.csv')\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the NaN values in the description with \"\" and NaN values in other columns with 0\n",
    "books['description'] = books['description'].fillna(\"\")\n",
    "books['title'] = books['description'].fillna(\"\")\n",
    "books = books.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-English books\n",
    "books = books[books['language_code'] == 'eng']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = books['description']\n",
    "\n",
    "descriptions = descriptions.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map tags to ones that the lemmatiser will understand.\n",
    "\n",
    "tag_map = defaultdict(lambda : \"n\")  # by default, assume nouns\n",
    "tag_map['J'] = \"a\"  # adjectives\n",
    "tag_map['V'] = \"v\"  # verbs\n",
    "tag_map['R'] = \"r\"  # adverbs\n",
    "\n",
    "# Create a function to get the pos tags for a set of tokens, and return the tokens in a way a\n",
    "# lemmatizer can interpret\n",
    "def get_wordnet_tags(tokens):\n",
    "    \"\"\"Gets wordnet tags from a set of tokens\n",
    "    \n",
    "       Input:\n",
    "           - a list of string tokens\n",
    "       Output:\n",
    "           - a list of (word, tag) tuples\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tag tokens with pos_tagger\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "    # Convert each tag to a version wordnet can understand\n",
    "    tagged_tokens = [(token[0], tag_map[token[1][0]]) for token in tagged_tokens]\n",
    "    \n",
    "    return tagged_tokens\n",
    "\n",
    "# Create a lemmatizing object\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Tag each token in each description\n",
    "\n",
    "descriptions = descriptions.apply(get_wordnet_tags)\n",
    "\n",
    "# Lemmatize the sets of tokens; this code takes a while to run\n",
    "\n",
    "descriptions = descriptions.apply(lambda tokens: [lemma.lemmatize(word=token[0],\n",
    "                                                                  pos=token[1])\n",
    "                                                  for token in tokens])\n",
    "\n",
    "\n",
    "# Get a list of stopwords\n",
    "\n",
    "stops = stopwords.words(\"english\")\n",
    "\n",
    "# Filter out all stopwords and words less than 3 letters long from the descriptions.\n",
    "\n",
    "descriptions = descriptions.apply(lambda tokens: [word.lower() for word in tokens\n",
    "                                                  if word not in stops\n",
    "                                                  and len(word) > 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the modified descriptions back into the dataframe\n",
    "books['modified_description'] = descriptions\n",
    "books.head(20)\n",
    "\n",
    "descriptions = books[\"modified_description\"].apply(lambda text: \" \".join(text))\n",
    "\n",
    "# Save the dataframe to a csv file\n",
    "books.to_csv('Datasets/goodreads_books_modified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tf-idf matrix for each modified description\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, min_df=0.01)\n",
    "\n",
    "description_dtm = tfidf_vectorizer.fit_transform(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that processes a text description into the same format as the provided descriptions.\n",
    "\n",
    "def convert_text_to_vector(text):\n",
    "    \"\"\"Converts a text string into a TFIDF vector\n",
    "    \n",
    "       Input:\n",
    "           - text (str): a book description\n",
    "       Output:\n",
    "           - vector (scipy sparse matrix): a tf-idf vector for the description\n",
    "    \"\"\"\n",
    "    # Clean text\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^a-z ]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # Lemmatize and remove stopwords\n",
    "    text = text.split(\" \")\n",
    "    text = get_wordnet_tags(text)\n",
    "    text = [lemma.lemmatize(word=word[0], pos=word[1]) for word in text]\n",
    "    text = [word for word in text if word not in stops and len(word) > 3]\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    # Convert the description to a TF-IDF vector\n",
    "    vector = tfidf_vectorizer.transform([text])\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179188</th>\n",
       "      <td>The true story as told by a mother and daughte...</td>\n",
       "      <td>0.427919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109371</th>\n",
       "      <td>The daughter of esteemed writer Paula Fox and ...</td>\n",
       "      <td>0.366478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106773</th>\n",
       "      <td>Now Martini delivers Paul Madriani's most chal...</td>\n",
       "      <td>0.353470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448931</th>\n",
       "      <td>Duke Leto Atreides is now the skilful ruler of...</td>\n",
       "      <td>0.342531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292067</th>\n",
       "      <td>Dedicated to mothers, daughters and sons whose...</td>\n",
       "      <td>0.324415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118996</th>\n",
       "      <td>A witty and irresistible story of a mother and...</td>\n",
       "      <td>0.321737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5595</th>\n",
       "      <td>When Jess's daughter, Anna, is reported lost i...</td>\n",
       "      <td>0.314948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343346</th>\n",
       "      <td>When Jess's daughter, Anna, is reported lost i...</td>\n",
       "      <td>0.314948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264353</th>\n",
       "      <td>After her husband takes a concubine, Madame Li...</td>\n",
       "      <td>0.312969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101648</th>\n",
       "      <td>The first volume of the book series \"Successfu...</td>\n",
       "      <td>0.312871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  similarity\n",
       "179188  The true story as told by a mother and daughte...    0.427919\n",
       "109371  The daughter of esteemed writer Paula Fox and ...    0.366478\n",
       "106773  Now Martini delivers Paul Madriani's most chal...    0.353470\n",
       "448931  Duke Leto Atreides is now the skilful ruler of...    0.342531\n",
       "292067  Dedicated to mothers, daughters and sons whose...    0.324415\n",
       "118996  A witty and irresistible story of a mother and...    0.321737\n",
       "5595    When Jess's daughter, Anna, is reported lost i...    0.314948\n",
       "343346  When Jess's daughter, Anna, is reported lost i...    0.314948\n",
       "264353  After her husband takes a concubine, Madame Li...    0.312969\n",
       "101648  The first volume of the book series \"Successfu...    0.312871"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example description string \n",
    "\n",
    "test_description = \"\"\"Brittany K. Barnett was only a law student when she came across the case that would change her life forever—that of Sharanda Jones, single mother, business owner, and, like Brittany, Black daughter of the rural South. A victim of America’s devastating war on drugs, Sharanda had been torn away from her young daughter and was serving a life sentence without parole—for a first-time drug offense. In Sharanda, Brittany saw haunting echoes of her own life, as the daughter of a formerly incarcerated mother. As she studied this case, a system came into focus in which widespread racial injustice forms the core of America’s addiction to incarceration. Moved by Sharanda’s plight, Brittany set to work to gain her freedom.\"\"\"\n",
    "\n",
    "# Convert the test description to a vector \n",
    "\n",
    "query_vector = convert_text_to_vector(test_description)\n",
    "\n",
    "\n",
    "# Use cosine similarity to find the most similar vectors to the test\n",
    "\n",
    "similarities = cosine_similarity(query_vector, description_dtm).flatten()\n",
    "\n",
    "books['similarity'] = similarities\n",
    "\n",
    "# Sort the books by similarity\n",
    "books.sort_values('similarity', ascending=False, inplace=True)\n",
    "\n",
    "# Print the top 10 most similar books\n",
    "books[['title', 'similarity']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the data\n",
    "path = \"Datasets/goodreads_interactions.csv\"\n",
    "\n",
    "# Define the chunksize for reading the data\n",
    "chunksize = 10 ** 6\n",
    "\n",
    "# Create a dataframe for user with id 661ff6b7041ea1935101f16846e3cba6 will need to use batch processing\n",
    "\n",
    "# Initialize reader object: reader\n",
    "user_id = '661ff6b7041ea1935101f16846e3cba6'\n",
    "\n",
    "user_df = pd.DataFrame()\n",
    "\n",
    "# Process the file in chunks\n",
    "with pd.read_csv(path, chunksize=chunksize) as reader:\n",
    "    for chunk in reader:\n",
    "        user_df = pd.concat([user_df, chunk[chunk['user_id'] == user_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              isbn language_code  \\\n",
      "316182  0312364121           eng   \n",
      "33541   1624300138           eng   \n",
      "64325   0987526146           eng   \n",
      "231961  1465341889           eng   \n",
      "383650  0525478817           eng   \n",
      "371529  1476753164           eng   \n",
      "196819  1455549002           eng   \n",
      "371680  1476763976           eng   \n",
      "\n",
      "                                              description         isbn13  \\\n",
      "316182  From the author of the smash-hit bestseller Fi...  9780312364120   \n",
      "33541   From the New York Times and USA Today bestsell...  9781624300134   \n",
      "64325   Ryan Kendall is broken. He understands pain. H...  9780987526144   \n",
      "231961  Every action has consequences.\\nWaking in an u...  9781465341884   \n",
      "383650  There is an alternate cover edition \u0001.\\n\"I fel...  9780525478812   \n",
      "371529  From #1 New York Timesbestselling author Colle...  9781476753164   \n",
      "196819  Five months ago, Camryn and Andrew, both deali...  9781455549009   \n",
      "371680  A Chicago reporter in her mid-twenties unexpec...  9781476763972   \n",
      "\n",
      "         book_id                                              title  \\\n",
      "316182   6668467  From the author of the smash-hit bestseller Fi...   \n",
      "33541   18467189  From the New York Times and USA Today bestsell...   \n",
      "64325   18304765  Ryan Kendall is broken. He understands pain. H...   \n",
      "231961  12368985  Every action has consequences.\\nWaking in an u...   \n",
      "383650  11870085  There is an alternate cover edition \u0001.\\n\"I fel...   \n",
      "371529  18143950  From #1 New York Timesbestselling author Colle...   \n",
      "196819  17899696  Five months ago, Camryn and Andrew, both deali...   \n",
      "371680  18280662  A Chicago reporter in her mid-twenties unexpec...   \n",
      "\n",
      "        num_pages                               modified_description  \\\n",
      "316182        394  [from, author, smash-hit, bestseller, firefly,...   \n",
      "33541         320  [from, new, york, times, usa, today, bestselli...   \n",
      "64325         346  [ryan, kendall, break, understand, pain, know,...   \n",
      "231961        368  [every, action, consequence, waking, unfamilia...   \n",
      "383650        313  [there, alternate, cover, edition, fell, love,...   \n",
      "371529        367  [from, new, york, timesbestselling, author, co...   \n",
      "196819        412  [five, month, ago, camryn, andrew, deal, perso...   \n",
      "371680        320  [chicago, reporter, mid-twenties, unexpectedly...   \n",
      "\n",
      "        similarity  \n",
      "316182    0.184981  \n",
      "33541     0.105421  \n",
      "64325     0.035403  \n",
      "231961    0.035221  \n",
      "383650    0.031636  \n",
      "371529    0.025728  \n",
      "196819    0.023872  \n",
      "371680    0.008061  \n"
     ]
    }
   ],
   "source": [
    "# Get the descriptions for the books the user has read\n",
    "user_books = user_df['book_id'].unique()\n",
    "user_books = books[books['book_id'].isin(user_books)]\n",
    "print(user_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.056290355227640046\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean similarty between the provided description and the descriptions of the books the user has read\n",
    "mean_similarity = user_books['similarity'].mean()\n",
    "print(mean_similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
