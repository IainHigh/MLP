{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IainHigh/MLP/blob/main/rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OTMuHDo2kAMA"
      },
      "outputs": [],
      "source": [
        "# !pip install pytorch_pretrained_bert --quiet\n",
        "# !pip install datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RYp9bIhaz-XL"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import nltk\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import datasets\n",
        "import statistics\n",
        "import spacy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pprint import pprint\n",
        "from tqdm.notebook import tqdm\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "# from torchtext.vocab import Vectors\n",
        "# from transformers import AutoTokenizer\n",
        "import torch.optim as optim\n",
        "\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert import BertForMaskedLM\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import pos_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SX8u7FupDkZe",
        "outputId": "d1fb97af-1708-4e08-9c7c-f54b80b105bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xdFv-3pK8RK",
        "outputId": "b586e69d-e159-4bad-b952-1b899476434a"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "WXUaI60AmzUB"
      },
      "outputs": [],
      "source": [
        "# @title Set random seed\n",
        "\n",
        "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
        "\n",
        "# for DL its critical to set the random seed so that students can have a\n",
        "# baseline to compare their results to expected results.\n",
        "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
        "\n",
        "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=None, seed_torch=True):\n",
        "  \"\"\"\n",
        "  Handles variability by controlling sources of randomness\n",
        "  through set seed values\n",
        "\n",
        "  Args:\n",
        "    seed: Integer\n",
        "      Set the seed value to given integer.\n",
        "      If no seed, set seed value to random integer in the range 2^32\n",
        "    seed_torch: Bool\n",
        "      Seeds the random number generator for all devices to\n",
        "      offer some guarantees on reproducibility\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "\n",
        "# In case that `DataLoader` is used\n",
        "def seed_worker(worker_id):\n",
        "  \"\"\"\n",
        "  DataLoader will reseed workers following randomness in\n",
        "  multi-process data loading algorithm.\n",
        "\n",
        "  Args:\n",
        "    worker_id: integer\n",
        "      ID of subprocess to seed. 0 means that\n",
        "      the data will be loaded in the main process\n",
        "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "3DvGFFsnmfPb"
      },
      "outputs": [],
      "source": [
        "# @title Set device (GPU or CPU). Execute `set_device()`\n",
        "# especially if torch modules used.\n",
        "\n",
        "# inform the user if the notebook uses GPU or CPU.\n",
        "\n",
        "def set_device():\n",
        "  \"\"\"\n",
        "  Set the device. CUDA if available, CPU otherwise\n",
        "\n",
        "  Args:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"WARNING: For this notebook to perform best, \"\n",
        "        \"if possible, in the menu under `Runtime` -> \"\n",
        "        \"`Change runtime type.`  select `GPU` \")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook.\")\n",
        "\n",
        "  return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WhoivcCmjAW",
        "outputId": "76d1c4da-0445-4f5c-effe-d4752f9da37b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random seed 42 has been set.\n",
            "WARNING: For this notebook to perform best, if possible, in the menu under `Runtime` -> `Change runtime type.`  select `GPU` \n"
          ]
        }
      ],
      "source": [
        "SEED = 42\n",
        "set_seed(seed=SEED)\n",
        "DEVICE = set_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5mt_4xYD5EUo"
      },
      "outputs": [],
      "source": [
        "# google colab paths\n",
        "# train_csv_path = \"/content/drive/MyDrive/mlp-project/data/train_interactions.csv\"\n",
        "# val_csv_path = \"/content/drive/MyDrive/mlp-project/data/test_interactions.csv\"\n",
        "# test_csv_path = \"/content/drive/MyDrive/mlp-project/data/validate_interactions.csv\"\n",
        "\n",
        "# local paths\n",
        "train_csv_path = \"./Datasets/train_interactions.csv\"\n",
        "test_csv_path = \"./Datasets/test_interactions.csv\"\n",
        "val_csv_path = \"./Datasets/validate_interactions.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "V0WcjJFPjzeE"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(train_csv_path)\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "val_df = pd.read_csv(val_csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmv-QHLKnOdt",
        "outputId": "1af2fa39-e707-460b-d5e5-ff6fd7560676"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(19719025, 5)\n"
          ]
        }
      ],
      "source": [
        "print(train_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "zAPoPMnEr1gM",
        "outputId": "98596c42-199b-440c-a077-b90186bebfd5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>book_id</th>\n",
              "      <th>date_added</th>\n",
              "      <th>read_at</th>\n",
              "      <th>started_at</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17277ab9d32482da501c252052235561</td>\n",
              "      <td>25695484</td>\n",
              "      <td>2017-08-15 14:42:19</td>\n",
              "      <td>2017-08-17 17:12:49</td>\n",
              "      <td>2017-08-15 14:42:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17277ab9d32482da501c252052235561</td>\n",
              "      <td>23411534</td>\n",
              "      <td>2017-08-15 14:40:37</td>\n",
              "      <td>2017-03-01 00:00:00</td>\n",
              "      <td>2017-03-01 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>17277ab9d32482da501c252052235561</td>\n",
              "      <td>16130</td>\n",
              "      <td>2017-04-21 19:41:34</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2017-04-21 19:41:34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17277ab9d32482da501c252052235561</td>\n",
              "      <td>17562818</td>\n",
              "      <td>2016-10-20 10:52:14</td>\n",
              "      <td>2016-10-22 00:00:00</td>\n",
              "      <td>2016-10-20 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17277ab9d32482da501c252052235561</td>\n",
              "      <td>27066704</td>\n",
              "      <td>2016-08-10 17:00:25</td>\n",
              "      <td>2016-08-11 21:57:00</td>\n",
              "      <td>2016-08-10 17:00:25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            user_id   book_id           date_added  \\\n",
              "0  17277ab9d32482da501c252052235561  25695484  2017-08-15 14:42:19   \n",
              "1  17277ab9d32482da501c252052235561  23411534  2017-08-15 14:40:37   \n",
              "2  17277ab9d32482da501c252052235561     16130  2017-04-21 19:41:34   \n",
              "3  17277ab9d32482da501c252052235561  17562818  2016-10-20 10:52:14   \n",
              "4  17277ab9d32482da501c252052235561  27066704  2016-08-10 17:00:25   \n",
              "\n",
              "               read_at           started_at  \n",
              "0  2017-08-17 17:12:49  2017-08-15 14:42:21  \n",
              "1  2017-03-01 00:00:00  2017-03-01 00:00:00  \n",
              "2                  NaN  2017-04-21 19:41:34  \n",
              "3  2016-10-22 00:00:00  2016-10-20 00:00:00  \n",
              "4  2016-08-11 21:57:00  2016-08-10 17:00:25  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ia9jqKMMNRcM"
      },
      "outputs": [],
      "source": [
        "# book_csv_path = \"/content/drive/MyDrive/mlp-project/data/books_filtered_by_language.csv\"\n",
        "book_csv_path = \"./Datasets/books_filtered_by_language.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TnD-6qymCLrh"
      },
      "outputs": [],
      "source": [
        "book_data_df = pd.read_csv(book_csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txS6K4LRCQWD",
        "outputId": "24cddad4-96c2-414f-f026-0984adb386d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(434423, 7)\n"
          ]
        }
      ],
      "source": [
        "print(book_data_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "id": "1vR229QGCz5v",
        "outputId": "b01b0004-ac64-48d9-e17a-f4966fbbdf30"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isbn</th>\n",
              "      <th>language_code</th>\n",
              "      <th>description</th>\n",
              "      <th>isbn13</th>\n",
              "      <th>book_id</th>\n",
              "      <th>title</th>\n",
              "      <th>num_pages</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0743294297</td>\n",
              "      <td>eng</td>\n",
              "      <td>Addie Downs and Valerie Adler were eight when ...</td>\n",
              "      <td>9780743294294</td>\n",
              "      <td>6066819</td>\n",
              "      <td>Best Friends Forever</td>\n",
              "      <td>368.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>eng</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>33394837</td>\n",
              "      <td>The House of Memory (Pluto's Snitch #2)</td>\n",
              "      <td>318.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>eng</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9781621086949</td>\n",
              "      <td>21401188</td>\n",
              "      <td>Glimmering Light</td>\n",
              "      <td>160.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>eng</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30227122</td>\n",
              "      <td>The 30s (Fantastic Films of the Decades, #2)</td>\n",
              "      <td>255.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1479174661</td>\n",
              "      <td>eng</td>\n",
              "      <td>Arrianna Williams is an ordinary 25 yr. old wo...</td>\n",
              "      <td>9781479174669</td>\n",
              "      <td>16037548</td>\n",
              "      <td>Untold Secrets: Fire &amp; Ice</td>\n",
              "      <td>168.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1483985644</td>\n",
              "      <td>eng</td>\n",
              "      <td>Embrace the word of God with the inspirational...</td>\n",
              "      <td>9781483985640</td>\n",
              "      <td>18628482</td>\n",
              "      <td>Understand God's Word - Walk in the Truth</td>\n",
              "      <td>412.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>9780807843</td>\n",
              "      <td>eng</td>\n",
              "      <td>\"This critical, historical, and theoretical st...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13598461</td>\n",
              "      <td>Labor and Desire: Women's Revolutionary Fictio...</td>\n",
              "      <td>236.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0980145988</td>\n",
              "      <td>eng</td>\n",
              "      <td>Elfin mercenaries, Lark and her brother Orin, ...</td>\n",
              "      <td>9780980145984</td>\n",
              "      <td>13598465</td>\n",
              "      <td>Faminelands: The Carp's Eye (Book 1)</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1850294607</td>\n",
              "      <td>eng</td>\n",
              "      <td>Restore, revamp, repair, and revitalize your h...</td>\n",
              "      <td>9781850294603</td>\n",
              "      <td>427479</td>\n",
              "      <td>Terence Conran's DIY By Design: Over 30 Projec...</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NaN</td>\n",
              "      <td>eng</td>\n",
              "      <td>The questions plaguing Captain America's dream...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13571772</td>\n",
              "      <td>Captain America: Winter Soldier (The Ultimate ...</td>\n",
              "      <td>146.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         isbn language_code  \\\n",
              "0  0743294297           eng   \n",
              "1         NaN           eng   \n",
              "2         NaN           eng   \n",
              "3         NaN           eng   \n",
              "4  1479174661           eng   \n",
              "5  1483985644           eng   \n",
              "6  9780807843           eng   \n",
              "7  0980145988           eng   \n",
              "8  1850294607           eng   \n",
              "9         NaN           eng   \n",
              "\n",
              "                                         description         isbn13   book_id  \\\n",
              "0  Addie Downs and Valerie Adler were eight when ...  9780743294294   6066819   \n",
              "1                                                NaN            NaN  33394837   \n",
              "2                                                NaN  9781621086949  21401188   \n",
              "3                                                NaN            NaN  30227122   \n",
              "4  Arrianna Williams is an ordinary 25 yr. old wo...  9781479174669  16037548   \n",
              "5  Embrace the word of God with the inspirational...  9781483985640  18628482   \n",
              "6  \"This critical, historical, and theoretical st...            NaN  13598461   \n",
              "7  Elfin mercenaries, Lark and her brother Orin, ...  9780980145984  13598465   \n",
              "8  Restore, revamp, repair, and revitalize your h...  9781850294603    427479   \n",
              "9  The questions plaguing Captain America's dream...            NaN  13571772   \n",
              "\n",
              "                                               title  num_pages  \n",
              "0                               Best Friends Forever      368.0  \n",
              "1            The House of Memory (Pluto's Snitch #2)      318.0  \n",
              "2                                   Glimmering Light      160.0  \n",
              "3       The 30s (Fantastic Films of the Decades, #2)      255.0  \n",
              "4                         Untold Secrets: Fire & Ice      168.0  \n",
              "5          Understand God's Word - Walk in the Truth      412.0  \n",
              "6  Labor and Desire: Women's Revolutionary Fictio...      236.0  \n",
              "7               Faminelands: The Carp's Eye (Book 1)      100.0  \n",
              "8  Terence Conran's DIY By Design: Over 30 Projec...      256.0  \n",
              "9  Captain America: Winter Soldier (The Ultimate ...      146.0  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "book_data_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "opIgHCqF608f"
      },
      "outputs": [],
      "source": [
        "# mod_book_csv_path = \"/content/drive/MyDrive/mlp-project/data/books_filtered_by_language_modified_desc.csv\"\n",
        "mod_book_csv_path = \"./Datasets/books_filtered_by_language_modified_desc.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4QZwiWe6Bwe5"
      },
      "outputs": [],
      "source": [
        "mod_book_data_df = pd.read_csv(mod_book_csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0gAEPlNr6Q3",
        "outputId": "b803bd82-e9c2-42e1-e749-3e790d687e32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(406913, 8)\n"
          ]
        }
      ],
      "source": [
        "print(mod_book_data_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "X8VOBSi5CNwC",
        "outputId": "9f335956-cf30-45f5-c5ca-bd62a18fbcf5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isbn</th>\n",
              "      <th>language_code</th>\n",
              "      <th>description</th>\n",
              "      <th>isbn13</th>\n",
              "      <th>book_id</th>\n",
              "      <th>title</th>\n",
              "      <th>num_pages</th>\n",
              "      <th>modified_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0743294297</td>\n",
              "      <td>eng</td>\n",
              "      <td>Addie Downs and Valerie Adler were eight when ...</td>\n",
              "      <td>9780743294294</td>\n",
              "      <td>6066819</td>\n",
              "      <td>Best Friends Forever</td>\n",
              "      <td>368.0</td>\n",
              "      <td>addie downs valerie adler eight first meet dec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1479174661</td>\n",
              "      <td>eng</td>\n",
              "      <td>Arrianna Williams is an ordinary 25 yr. old wo...</td>\n",
              "      <td>9781479174669</td>\n",
              "      <td>16037548</td>\n",
              "      <td>Untold Secrets: Fire &amp; Ice</td>\n",
              "      <td>168.0</td>\n",
              "      <td>arrianna williams ordinary yr. old woman think...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1483985644</td>\n",
              "      <td>eng</td>\n",
              "      <td>Embrace the word of God with the inspirational...</td>\n",
              "      <td>9781483985640</td>\n",
              "      <td>18628482</td>\n",
              "      <td>Understand God's Word - Walk in the Truth</td>\n",
              "      <td>412.0</td>\n",
              "      <td>embrace word god inspirational book understand...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9780807843</td>\n",
              "      <td>eng</td>\n",
              "      <td>\"This critical, historical, and theoretical st...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13598461</td>\n",
              "      <td>Labor and Desire: Women's Revolutionary Fictio...</td>\n",
              "      <td>236.0</td>\n",
              "      <td>critical historical theoretical study look lit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0980145988</td>\n",
              "      <td>eng</td>\n",
              "      <td>Elfin mercenaries, Lark and her brother Orin, ...</td>\n",
              "      <td>9780980145984</td>\n",
              "      <td>13598465</td>\n",
              "      <td>Faminelands: The Carp's Eye (Book 1)</td>\n",
              "      <td>100.0</td>\n",
              "      <td>elfin mercenary lark brother orin hunt perfect...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         isbn language_code  \\\n",
              "0  0743294297           eng   \n",
              "1  1479174661           eng   \n",
              "2  1483985644           eng   \n",
              "3  9780807843           eng   \n",
              "4  0980145988           eng   \n",
              "\n",
              "                                         description         isbn13   book_id  \\\n",
              "0  Addie Downs and Valerie Adler were eight when ...  9780743294294   6066819   \n",
              "1  Arrianna Williams is an ordinary 25 yr. old wo...  9781479174669  16037548   \n",
              "2  Embrace the word of God with the inspirational...  9781483985640  18628482   \n",
              "3  \"This critical, historical, and theoretical st...            NaN  13598461   \n",
              "4  Elfin mercenaries, Lark and her brother Orin, ...  9780980145984  13598465   \n",
              "\n",
              "                                               title  num_pages  \\\n",
              "0                               Best Friends Forever      368.0   \n",
              "1                         Untold Secrets: Fire & Ice      168.0   \n",
              "2          Understand God's Word - Walk in the Truth      412.0   \n",
              "3  Labor and Desire: Women's Revolutionary Fictio...      236.0   \n",
              "4               Faminelands: The Carp's Eye (Book 1)      100.0   \n",
              "\n",
              "                                modified_description  \n",
              "0  addie downs valerie adler eight first meet dec...  \n",
              "1  arrianna williams ordinary yr. old woman think...  \n",
              "2  embrace word god inspirational book understand...  \n",
              "3  critical historical theoretical study look lit...  \n",
              "4  elfin mercenary lark brother orin hunt perfect...  "
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mod_book_data_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht6L5X5kUVIJ",
        "outputId": "e6fb8e71-10f9-4b11-bd7a-8367f7999e9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "addie downs valerie adler eight first meet decide best friend forever wake tragedy betrayal teenage year everything change val fame fortune addie stay behind small midwestern town destiny however store two twenty-five year later val show addie front door blood coat terror face beginning wild adventure two woman join love history find strength together could find alone\n",
            "arrianna williams ordinary yr. old woman think stumble across special book soon see special book really least think know book come life arrianna longer read page story shortly read book run old childhood friend damian quickly fall last long hurt find comfort another man thinking life completely normal soon find special gift put life danger archangel name gabriel come rescue turn thing get bad hunt gift man seek comfort desperate need save life meanwhile feeling gabriel grow stronger wonder could true love could angel ever love way love defeat fall angel demon many secret many lie much heartache happen journey discover really mean\n",
            "embrace word god inspirational book understand god word walk truth powerful look lord instruction warning day judgment act trumpet time restoration mean help people understand god word follow path truth lord open first seal send angel white horse lead army restore whole land sound trumpet proclaim time restoration come lord speak people holy bible make understand word reveal command make walk truth lead believer church repent forgive sin ultimately determine bring back chosen people let enter kingdom heaven organized forty different chapter chapter divide two part first part consist lord instruction warning aim show people walk truth second part summarize key passage holy bible help reader truly understand god word chapter demonstrate wish enter kingdom heaven must obey lord command thorough understanding holy bible need order ms. zhang yun call lord work book february 2012 lord reveal raise faithful servant whole land holy work together command share free ebook believer contain first part chapter readers may official website walktruth.com download would love paperback read may purchase amazon.com\n",
            "critical historical theoretical study look little-known group novel write 1930s woman literary radical arguing class consciousness figure metaphor gender paula rabinowitz challenge conventional wisdom feminism discourse disappear decade focus way sexuality maternity reconstruct classic proletarian novel speak working-class woman radical female intellectual book via amazon\n",
            "elfin mercenary lark brother orin hunt perfect pearl know carp eye\n"
          ]
        }
      ],
      "source": [
        "for text in list(mod_book_data_df.head()['modified_description']):\n",
        "  print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "anROCBAxUoRf"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Tokenization\n",
        "tok = spacy.load(\"en_core_web_sm\")\n",
        "def tokenize (text):\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
        "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
        "    nopunct = regex.sub(\" \", text.lower())\n",
        "    return [token.text for token in tok.tokenizer(nopunct)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd6Kqv_9U6CK",
        "outputId": "7b36fc63-9df1-4648-c20c-0e338f1d623b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['addie', 'downs', 'valerie', 'adler', 'eight', 'first', 'meet', 'decide', 'best', 'friend', 'forever', 'wake', 'tragedy', 'betrayal', 'teenage', 'year', 'everything', 'change', 'val', 'fame', 'fortune', 'addie', 'stay', 'behind', 'small', 'midwestern', 'town', 'destiny', 'however', 'store', 'two', 'twenty', 'five', 'year', 'later', 'val', 'show', 'addie', 'front', 'door', 'blood', 'coat', 'terror', 'face', 'beginning', 'wild', 'adventure', 'two', 'woman', 'join', 'love', 'history', 'find', 'strength', 'together', 'could', 'find', 'alone']\n",
            "['arrianna', 'williams', 'ordinary', 'yr', ' ', 'old', 'woman', 'think', 'stumble', 'across', 'special', 'book', 'soon', 'see', 'special', 'book', 'really', 'least', 'think', 'know', 'book', 'come', 'life', 'arrianna', 'longer', 'read', 'page', 'story', 'shortly', 'read', 'book', 'run', 'old', 'childhood', 'friend', 'damian', 'quickly', 'fall', 'last', 'long', 'hurt', 'find', 'comfort', 'another', 'man', 'thinking', 'life', 'completely', 'normal', 'soon', 'find', 'special', 'gift', 'put', 'life', 'danger', 'archangel', 'name', 'gabriel', 'come', 'rescue', 'turn', 'thing', 'get', 'bad', 'hunt', 'gift', 'man', 'seek', 'comfort', 'desperate', 'need', 'save', 'life', 'meanwhile', 'feeling', 'gabriel', 'grow', 'stronger', 'wonder', 'could', 'true', 'love', 'could', 'angel', 'ever', 'love', 'way', 'love', 'defeat', 'fall', 'angel', 'demon', 'many', 'secret', 'many', 'lie', 'much', 'heartache', 'happen', 'journey', 'discover', 'really', 'mean']\n",
            "['embrace', 'word', 'god', 'inspirational', 'book', 'understand', 'god', 'word', 'walk', 'truth', 'powerful', 'look', 'lord', 'instruction', 'warning', 'day', 'judgment', 'act', 'trumpet', 'time', 'restoration', 'mean', 'help', 'people', 'understand', 'god', 'word', 'follow', 'path', 'truth', 'lord', 'open', 'first', 'seal', 'send', 'angel', 'white', 'horse', 'lead', 'army', 'restore', 'whole', 'land', 'sound', 'trumpet', 'proclaim', 'time', 'restoration', 'come', 'lord', 'speak', 'people', 'holy', 'bible', 'make', 'understand', 'word', 'reveal', 'command', 'make', 'walk', 'truth', 'lead', 'believer', 'church', 'repent', 'forgive', 'sin', 'ultimately', 'determine', 'bring', 'back', 'chosen', 'people', 'let', 'enter', 'kingdom', 'heaven', 'organized', 'forty', 'different', 'chapter', 'chapter', 'divide', 'two', 'part', 'first', 'part', 'consist', 'lord', 'instruction', 'warning', 'aim', 'show', 'people', 'walk', 'truth', 'second', 'part', 'summarize', 'key', 'passage', 'holy', 'bible', 'help', 'reader', 'truly', 'understand', 'god', 'word', 'chapter', 'demonstrate', 'wish', 'enter', 'kingdom', 'heaven', 'must', 'obey', 'lord', 'command', 'thorough', 'understanding', 'holy', 'bible', 'need', 'order', 'ms', ' ', 'zhang', 'yun', 'call', 'lord', 'work', 'book', 'february', '     ', 'lord', 'reveal', 'raise', 'faithful', 'servant', 'whole', 'land', 'holy', 'work', 'together', 'command', 'share', 'free', 'ebook', 'believer', 'contain', 'first', 'part', 'chapter', 'readers', 'may', 'official', 'website', 'walktruth', 'com', 'download', 'would', 'love', 'paperback', 'read', 'may', 'purchase', 'amazon', 'com']\n",
            "['critical', 'historical', 'theoretical', 'study', 'look', 'little', 'known', 'group', 'novel', 'write', '    ', 's', 'woman', 'literary', 'radical', 'arguing', 'class', 'consciousness', 'figure', 'metaphor', 'gender', 'paula', 'rabinowitz', 'challenge', 'conventional', 'wisdom', 'feminism', 'discourse', 'disappear', 'decade', 'focus', 'way', 'sexuality', 'maternity', 'reconstruct', 'classic', 'proletarian', 'novel', 'speak', 'working', 'class', 'woman', 'radical', 'female', 'intellectual', 'book', 'via', 'amazon']\n",
            "['elfin', 'mercenary', 'lark', 'brother', 'orin', 'hunt', 'perfect', 'pearl', 'know', 'carp', 'eye']\n"
          ]
        }
      ],
      "source": [
        "for text in list(mod_book_data_df.head()['modified_description']):\n",
        "  print(tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "addie downs valerie adler eight first meet decide best friend forever wake tragedy betrayal teenage year everything change val fame fortune addie stay behind small midwestern town destiny however store two twenty-five year later val show addie front door blood coat terror face beginning wild adventure two woman join love history find strength together could find alone\n",
            "arrianna williams ordinary yr. old woman think stumble across special book soon see special book really least think know book come life arrianna longer read page story shortly read book run old childhood friend damian quickly fall last long hurt find comfort another man thinking life completely normal soon find special gift put life danger archangel name gabriel come rescue turn thing get bad hunt gift man seek comfort desperate need save life meanwhile feeling gabriel grow stronger wonder could true love could angel ever love way love defeat fall angel demon many secret many lie much heartache happen journey discover really mean\n",
            "embrace word god inspirational book understand god word walk truth powerful look lord instruction warning day judgment act trumpet time restoration mean help people understand god word follow path truth lord open first seal send angel white horse lead army restore whole land sound trumpet proclaim time restoration come lord speak people holy bible make understand word reveal command make walk truth lead believer church repent forgive sin ultimately determine bring back chosen people let enter kingdom heaven organized forty different chapter chapter divide two part first part consist lord instruction warning aim show people walk truth second part summarize key passage holy bible help reader truly understand god word chapter demonstrate wish enter kingdom heaven must obey lord command thorough understanding holy bible need order ms. zhang yun call lord work book february 2012 lord reveal raise faithful servant whole land holy work together command share free ebook believer contain first part chapter readers may official website walktruth.com download would love paperback read may purchase amazon.com\n",
            "critical historical theoretical study look little-known group novel write 1930s woman literary radical arguing class consciousness figure metaphor gender paula rabinowitz challenge conventional wisdom feminism discourse disappear decade focus way sexuality maternity reconstruct classic proletarian novel speak working-class woman radical female intellectual book via amazon\n",
            "elfin mercenary lark brother orin hunt perfect pearl know carp eye\n"
          ]
        }
      ],
      "source": [
        "for text in list(mod_book_data_df.head()['modified_description']):\n",
        "  print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "mod_book_data_df = mod_book_data_df[mod_book_data_df['modified_description'].notna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isbn</th>\n",
              "      <th>language_code</th>\n",
              "      <th>description</th>\n",
              "      <th>isbn13</th>\n",
              "      <th>book_id</th>\n",
              "      <th>title</th>\n",
              "      <th>num_pages</th>\n",
              "      <th>modified_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [isbn, language_code, description, isbn13, book_id, title, num_pages, modified_description]\n",
              "Index: []"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mod_book_data_df[mod_book_data_df['modified_description'].isnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "GkqM4b-EC_0Q"
      },
      "outputs": [],
      "source": [
        "#add_modified_description(\"/content/goodreads_data/books_filtered_by_language.csv\", \"/content/goodreads_data/books_filtered_by_language_mod_desc.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tok = spacy.load(\"en_core_web_sm\")\n",
        "def tokenize (text):\n",
        "    words = text.split()\n",
        "    return [word.strip() for word in words if word.strip()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n"
          ]
        }
      ],
      "source": [
        "book_data = pd.read_csv(mod_book_csv_path)\n",
        "for text in list(book_data['modified_description']):\n",
        "    if isinstance(text, float):\n",
        "        print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "oDFkTJGaNMuI"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def preprocess_data(training_df, val_df, test_df):\n",
        "\n",
        "    print(\"\\n\\nPreprocessing data...\")\n",
        "\n",
        "    # Calculate the time_to_start for each interaction\n",
        "    training_df[\"time_to_start_seconds\"] = pd.to_datetime(training_df[\"started_at\"]) - pd.to_datetime(training_df[\"date_added\"])\n",
        "    val_df[\"time_to_start_seconds\"] = pd.to_datetime(val_df[\"started_at\"]) - pd.to_datetime(val_df[\"date_added\"])\n",
        "    test_df[\"time_to_start_seconds\"] = pd.to_datetime(test_df[\"started_at\"]) - pd.to_datetime(test_df[\"date_added\"])\n",
        "\n",
        "    training_df[\"time_to_start_seconds\"] = training_df[\"time_to_start_seconds\"].dt.total_seconds()\n",
        "    val_df[\"time_to_start_seconds\"] = val_df[\"time_to_start_seconds\"].dt.total_seconds()\n",
        "    test_df[\"time_to_start_seconds\"] = test_df[\"time_to_start_seconds\"].dt.total_seconds()\n",
        "\n",
        "    # Remove time_to_start_seconds that are less than or equal to 0\n",
        "    training_df = training_df[training_df[\"time_to_start_seconds\"] > 0]\n",
        "    val_df = val_df[val_df[\"time_to_start_seconds\"] > 0]\n",
        "    test_df = test_df[test_df[\"time_to_start_seconds\"] > 0]\n",
        "\n",
        "    # Read the book data csv\n",
        "    book_data = pd.read_csv(mod_book_csv_path)\n",
        "\n",
        "    # drop rows that have null in modified description column\n",
        "    book_data = book_data[book_data['modified_description'].notna()]\n",
        "\n",
        "    # Tokenization\n",
        "    tok = spacy.load(\"en_core_web_sm\")\n",
        "    def tokenize (text):\n",
        "        words = text.split()\n",
        "        return [word.strip() for word in words if word.strip()]\n",
        "\n",
        "    # Count number of occurences of each word\n",
        "    counts = Counter()\n",
        "    for text in list(book_data['modified_description']):\n",
        "      counts.update(tokenize(text))\n",
        "      # counts.update(text)\n",
        "\n",
        "    # Deleting infrequent words\n",
        "    print(\"num_words before:\",len(counts.keys()))\n",
        "    for word in list(counts):\n",
        "        if counts[word] < 2:\n",
        "            del counts[word]\n",
        "    print(\"num_words after:\",len(counts.keys()))\n",
        "\n",
        "    # Creating vocabulary\n",
        "    vocab2index = {\"\":0, \"UNK\":1}\n",
        "    words = [\"\", \"UNK\"]\n",
        "    for word in counts:\n",
        "        vocab2index[word] = len(words)\n",
        "        words.append(word)\n",
        "\n",
        "    vocab_size = len(words)  # needed later in pytorch training loop\n",
        "\n",
        "    def encode_sentence(text, vocab2index, N=200):\n",
        "        tokenized = tokenize(text)\n",
        "        encoded = np.zeros(N, dtype=int)\n",
        "        enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
        "        #enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in text])\n",
        "        length = min(N, len(enc1))\n",
        "        encoded[:length] = enc1[:length]\n",
        "        return encoded, length\n",
        "\n",
        "    # Apply encoding function to each element in 'modified_description'\n",
        "    encoded_data = book_data['modified_description'].apply(lambda x: encode_sentence(x, vocab2index))\n",
        "\n",
        "    # Separate the arrays and lengths into separate columns\n",
        "    book_data['encoded'] = encoded_data.apply(lambda x: x[0])  # Extract arrays\n",
        "    book_data['encoded_length'] = encoded_data.apply(lambda x: x[1])  # Extract lengths\n",
        "\n",
        "    #book_data['encoded'] = book_data['modified_description'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\n",
        "\n",
        "    # Merge the book data with the training and test data\n",
        "    training_df = training_df.merge(book_data, how=\"left\", on=\"book_id\")\n",
        "    val_df = val_df.merge(book_data, how=\"left\", on=\"book_id\")\n",
        "    test_df = test_df.merge(book_data, how=\"left\", on=\"book_id\")\n",
        "\n",
        "\n",
        "     # Convert the language code into an integer\n",
        "    lang_map = {\n",
        "        \"eng\": 0,\n",
        "        \"en-US\": 1,\n",
        "        \"en-GB\": 2,\n",
        "    }\n",
        "\n",
        "    test_df[\"language_code\"] = test_df[\"language_code\"].map(lang_map)\n",
        "    val_df[\"language_code\"] = val_df[\"language_code\"].map(lang_map)\n",
        "    training_df[\"language_code\"] = training_df[\"language_code\"].map(lang_map)\n",
        "\n",
        "    # For each user, calculate the average time_to_start_seconds for all other books they have read\n",
        "    user_avg_time_to_start = training_df.groupby(\"user_id\")[\"time_to_start_seconds\"].mean()\n",
        "    user_avg_time_to_start = user_avg_time_to_start.rename(\"user_avg_time_to_start\")\n",
        "    training_df = training_df.merge(user_avg_time_to_start, how=\"left\", on=\"user_id\")\n",
        "\n",
        "    user_avg_time_to_start = val_df.groupby(\"user_id\")[\"time_to_start_seconds\"].mean()\n",
        "    user_avg_time_to_start = user_avg_time_to_start.rename(\"user_avg_time_to_start\")\n",
        "    val_df = val_df.merge(user_avg_time_to_start, how=\"left\", on=\"user_id\")\n",
        "\n",
        "    user_avg_time_to_start = test_df.groupby(\"user_id\")[\"time_to_start_seconds\"].mean()\n",
        "    user_avg_time_to_start = user_avg_time_to_start.rename(\"user_avg_time_to_start\")\n",
        "    test_df = test_df.merge(user_avg_time_to_start, how=\"left\", on=\"user_id\")\n",
        "\n",
        "    # Remove unnecessary columns - ones that are not useful for training the model\n",
        "    training_df = training_df.drop(\n",
        "        columns=[\"isbn\", \"isbn13\", \"date_added\", \"read_at\", \"started_at\", \"title\", \"description\"]\n",
        "    )\n",
        "    val_df = val_df.drop(\n",
        "        columns=[\"isbn\", \"isbn13\", \"date_added\", \"read_at\", \"started_at\", \"title\", \"description\"]\n",
        "    )\n",
        "    test_df = test_df.drop(\n",
        "        columns=[\"isbn\", \"isbn13\", \"date_added\", \"read_at\", \"started_at\", \"title\", \"description\"]\n",
        "    )\n",
        "\n",
        "    training_df = training_df.dropna()\n",
        "    val_df = val_df.dropna()\n",
        "    test_df = test_df.dropna()\n",
        "\n",
        "    return training_df, val_df, test_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGAo8GsDL8rQ",
        "outputId": "f501d8d5-fee9-49a8-c0ed-775c05f7dbd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Preprocessing data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15280\\1187580695.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  training_df[\"time_to_start_seconds\"] = pd.to_datetime(training_df[\"started_at\"]) - pd.to_datetime(training_df[\"date_added\"])\n",
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15280\\1187580695.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_df[\"time_to_start_seconds\"] = pd.to_datetime(val_df[\"started_at\"]) - pd.to_datetime(val_df[\"date_added\"])\n",
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15280\\1187580695.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_df[\"time_to_start_seconds\"] = pd.to_datetime(test_df[\"started_at\"]) - pd.to_datetime(test_df[\"date_added\"])\n",
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15280\\1187580695.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  training_df[\"time_to_start_seconds\"] = training_df[\"time_to_start_seconds\"].dt.total_seconds()\n",
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15280\\1187580695.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_df[\"time_to_start_seconds\"] = val_df[\"time_to_start_seconds\"].dt.total_seconds()\n",
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15280\\1187580695.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_df[\"time_to_start_seconds\"] = test_df[\"time_to_start_seconds\"].dt.total_seconds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_words before: 544854\n",
            "num_words after: 250227\n"
          ]
        }
      ],
      "source": [
        "processed_train_df, processed_val_df, processed_test_df = preprocess_data(train_df.head(10), val_df.head(10), test_df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0xFBggXl81yT"
      },
      "outputs": [],
      "source": [
        "# processed_train_df.to_csv(\"/content/drive/MyDrive/mlp-project/data/processed_train_df_2.csv\")\n",
        "# processed_val_df.to_csv(\"/content/drive/MyDrive/mlp-project/data/processed_val_df_2.csv\")\n",
        "# processed_test_df.to_csv(\"/content/drive/MyDrive/mlp-project/data/processed_test_df_2.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "EnaCs_eZMrYP",
        "outputId": "18106ca8-4205-444c-ffa0-991b8661d185"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>book_id</th>\n",
              "      <th>time_to_start_seconds</th>\n",
              "      <th>language_code</th>\n",
              "      <th>num_pages</th>\n",
              "      <th>modified_description</th>\n",
              "      <th>encoded</th>\n",
              "      <th>encoded_length</th>\n",
              "      <th>user_avg_time_to_start</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17277ab9d32482da501c252052235561</td>\n",
              "      <td>25695484</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>352.0</td>\n",
              "      <td>lucetta plum actress rise new york city force ...</td>\n",
              "      <td>[238957, 11735, 1108, 1323, 342, 347, 476, 767...</td>\n",
              "      <td>79</td>\n",
              "      <td>4.834320e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17277ab9d32482da501c252052235561</td>\n",
              "      <td>28696602</td>\n",
              "      <td>10846945.0</td>\n",
              "      <td>2</td>\n",
              "      <td>241.0</td>\n",
              "      <td>comedian activist hugely popular culture blogg...</td>\n",
              "      <td>[11923, 10850, 2525, 1236, 1828, 8364, 237885,...</td>\n",
              "      <td>92</td>\n",
              "      <td>4.834320e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>17277ab9d32482da501c252052235561</td>\n",
              "      <td>29056083</td>\n",
              "      <td>3656012.0</td>\n",
              "      <td>0</td>\n",
              "      <td>343.0</td>\n",
              "      <td>eighth story nineteen years later always diffi...</td>\n",
              "      <td>[6864, 73, 931, 3651, 33, 850, 747, 7291, 1078...</td>\n",
              "      <td>51</td>\n",
              "      <td>4.834320e+06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            user_id   book_id  time_to_start_seconds  \\\n",
              "0  17277ab9d32482da501c252052235561  25695484                    2.0   \n",
              "1  17277ab9d32482da501c252052235561  28696602             10846945.0   \n",
              "2  17277ab9d32482da501c252052235561  29056083              3656012.0   \n",
              "\n",
              "   language_code  num_pages  \\\n",
              "0              0      352.0   \n",
              "1              2      241.0   \n",
              "2              0      343.0   \n",
              "\n",
              "                                modified_description  \\\n",
              "0  lucetta plum actress rise new york city force ...   \n",
              "1  comedian activist hugely popular culture blogg...   \n",
              "2  eighth story nineteen years later always diffi...   \n",
              "\n",
              "                                             encoded  encoded_length  \\\n",
              "0  [238957, 11735, 1108, 1323, 342, 347, 476, 767...              79   \n",
              "1  [11923, 10850, 2525, 1236, 1828, 8364, 237885,...              92   \n",
              "2  [6864, 73, 931, 3651, 33, 850, 747, 7291, 1078...              51   \n",
              "\n",
              "   user_avg_time_to_start  \n",
              "0            4.834320e+06  \n",
              "1            4.834320e+06  \n",
              "2            4.834320e+06  "
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_train_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = list(processed_train_df['encoded'])\n",
        "X_train_enc_len = list(processed_train_df['encoded_length'])\n",
        "y_train = list(processed_train_df['time_to_start_seconds'])\n",
        "\n",
        "X_valid = list(processed_val_df['encoded'])\n",
        "X_valid_enc_len = list(processed_val_df['encoded_length'])\n",
        "y_valid = list(processed_val_df['time_to_start_seconds'])\n",
        "\n",
        "X_test = list(processed_test_df['encoded'])\n",
        "X_test_enc_len = list(processed_test_df['encoded_length'])\n",
        "y_test = list(processed_test_df['time_to_start_seconds'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CommonLitReadabiltyDataset(Dataset):\n",
        "    def __init__(self, X, Y, l):\n",
        "        self.X = X\n",
        "        self.y = Y\n",
        "        self.l = l\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx]), self.y[idx], self.l[idx]\n",
        "\n",
        "# X = list(processed_train_df['encoded'])\n",
        "# l = list(processed_train_df['encoded_length'])\n",
        "# y = list(processed_train_df['time_to_start_seconds'])\n",
        "\n",
        "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "train_ds = CommonLitReadabiltyDataset(X_train, y_train, X_train_enc_len)\n",
        "valid_ds = CommonLitReadabiltyDataset(X_valid, y_valid, X_valid_enc_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# for x, y, l in valid_ds:\n",
        "#         print('x:', x.long())\n",
        "#         print('y:', y)\n",
        "#         print('l:', l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw9U_J4zLNN-"
      },
      "source": [
        "## PyTorch training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "x_ujtYocC_on"
      },
      "outputs": [],
      "source": [
        "def train_model_regr(model, epochs=10, lr=0.001):\n",
        "\n",
        "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
        "    for i in range(epochs):\n",
        "        model.train()\n",
        "        sum_loss = 0.0\n",
        "        total = 0\n",
        "        for x, y, l in train_dl:\n",
        "            x = x.long()\n",
        "            #y = y.float()\n",
        "            y_pred = model(x, l)\n",
        "            optimizer.zero_grad()\n",
        "            loss = F.mse_loss(y_pred, y.unsqueeze(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            sum_loss += loss.item()*y.shape[0]\n",
        "            total += y.shape[0]\n",
        "        val_loss = validation_metrics_regr(model, val_dl)\n",
        "        if i % 5 == 1:\n",
        "            print(\"train mse %.3f val rmse %.3f\" % (sum_loss/total, val_loss))\n",
        "\n",
        "def validation_metrics_regr (model, valid_dl):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    sum_loss = 0.0\n",
        "    for x, y, l in valid_dl:\n",
        "        x = x.long()\n",
        "        #y = y.float()\n",
        "        y_hat = model(x, l)\n",
        "        loss = np.sqrt(F.mse_loss(y_hat, y.unsqueeze(-1)).item())\n",
        "        total += y.shape[0]\n",
        "        sum_loss += loss.item()*y.shape[0]\n",
        "    return sum_loss/total\n",
        "\n",
        "batch_size = 64\n",
        "# vocab_size = len(words)  # defined earlier\n",
        "embedding_dim = 300\n",
        "hidden_dim = 200\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_dl = DataLoader(valid_ds, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LSTM regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTM_regr(torch.nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        \n",
        "    def forward(self, x, l):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        lstm_out, (ht, ct) = self.lstm(x)\n",
        "        return self.linear(ht[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'vocab_size' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\User\\mlp-project\\rnn.ipynb Cell 43\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/mlp-project/rnn.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m  LSTM_regr(vocab_size, embedding_dim, hidden_dim)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'vocab_size' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "model =  LSTM_regr(vocab_size, embedding_dim, hidden_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMexiKDWjOfB16JkT/ilHW1",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
